{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Semi-Supervised Architope (Chaotic Data)\n",
    "---\n",
    "- This code Implements Algorithm 3.2 of the \"Architopes\" paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode: Code-Testin Parameter(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_run = True\n",
    "#pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta-parameters\n",
    "In Grid_Enhanced_NetworkGrid_Enhanced_Network.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test-size Ratio\n",
    "test_size_ratio = 1\n",
    "min_height = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------#\n",
    "# Only For Motivational Example Only #\n",
    "#------------------------------------#\n",
    "## Hyperparameters\n",
    "percentage_in_row = .25\n",
    "N = 5000\n",
    "\n",
    "def f_1(x):\n",
    "    return x\n",
    "def f_2(x):\n",
    "    return x**2\n",
    "x_0 = 0\n",
    "x_end = 1\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Only turn of if running code directly here, typically this script should be run be called by other notebooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "results_path = \"./outputs/models/\"\n",
    "results_tables_path = \"./outputs/results/\"\n",
    "raw_data_path_folder = \"./inputs/raw/\"\n",
    "data_path_folder = \"./inputs/data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep Feature Builder - Ready\n",
      "Deep Classifier - Ready\n",
      "lorenz input data               x         y          z\n",
      "0      5.894076 -6.415820  -8.194456\n",
      "1      4.821934 -4.429324  -8.263782\n",
      "2      4.022918 -2.797976  -8.203039\n",
      "3      3.441617 -1.433997  -8.064589\n",
      "4      3.035476 -0.265226  -7.879283\n",
      "...         ...       ...        ...\n",
      "47994  8.413575  7.664786  27.872319\n",
      "47995  8.339276  7.603378  27.769801\n",
      "47996  8.266649  7.551238  27.659791\n",
      "47997  8.196433  7.508691  27.543485\n",
      "47998  8.129324  7.475957  27.422103\n",
      "\n",
      "[47999 rows x 3 columns]\n",
      "size training input data: (43199, 3)\n",
      "size test input data: (4800, 3)\n",
      "size training output data: (43199, 3)\n",
      "size test output data: (4800, 3)\n",
      "#================================================#\n",
      " Training Datasize: 43199 and test datasize: 4800.  \n",
      "#================================================#\n",
      "lorenz_input_data shape: (47999, 3)\n",
      "size training output data: (43199, 3)\n",
      "size test output data: (4800, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load Packages/Modules\n",
    "exec(open('Init_Dump.py').read())\n",
    "# Load Hyper-parameter Grid - here is set the Option_Function\n",
    "exec(open('Grid_Enhanced_Network.py').read()) \n",
    "# Load Helper Function(s)\n",
    "exec(open('Helper_Functions.py').read())\n",
    "# Pre-process Data\n",
    "if Option_Function != \"Motivational_Example\": \n",
    "    exec(open('Chaos_Data_Preprocessor.py').read())\n",
    "else:\n",
    "    print(1)\n",
    "    exec(open('Motivational_Example.py').read())\n",
    "    print(\"Training Data size: \",X_train.shape[0])\n",
    "# Import time separately\n",
    "import time\n",
    "\n",
    "# TEMP\n",
    "# import pickle_compat\n",
    "# pickle_compat.patch()\n",
    "# param_grid_Vanilla_Nets['input_dim']=X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lorenz\n"
     ]
    }
   ],
   "source": [
    "print(Option_Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "tf.random.set_seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-Process:\n",
    "- Convert Categorical Variables to Dummies\n",
    "- Remove Bad Column\n",
    "- Perform Training/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Lipschitz Partition Builder\n",
    "\n",
    "We implement the random paritioning method of [Yair Bartal](https://scholar.google.com/citations?user=eCXP24kAAAAJ&hl=en):\n",
    "- [On approximating arbitrary metrices by tree metrics](https://dl.acm.org/doi/10.1145/276698.276725)\n",
    "\n",
    "The algorithm is summarized as follow:\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm:\n",
    " 1. Sample $\\alpha \\in [4^{-1},2^{-1}]$ randomly and uniformly,\n",
    " 2. Apply a random suffle of the data, (a random bijection $\\pi:\\{i\\}_{i=1}^X \\rightarrow \\mathbb{X}$),\n",
    " 3. For $i = 1,\\dots,I$:\n",
    "   - Set $K_i\\triangleq B\\left(\\pi(i),\\alpha \\Delta \\right) - \\bigcup_{j=1}^{i-1} P_j$\n",
    " \n",
    " 4. Remove empty members of $\\left\\{K_i\\right\\}_{i=1}^X$.  \n",
    " \n",
    " **Return**: $\\left\\{K_i\\right\\}_{i=1}^{\\tilde{X}}$.  \n",
    " \n",
    " For more details on the random-Lipschitz partition of Yair Bartal, see this [well-written blog post](https://nickhar.wordpress.com/2012/03/26/lecture-22-random-partitions-of-metric-spaces/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Random Partition Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use $\\Delta_{in} = Q_{q}\\left(\\Delta(\\mathbb{X})\\right)$ where $\\Delta(\\mathbb{X})$ is the vector of (Euclidean) distances between the given data-points, $q \\in (0,1)$ is a hyper-parameter, and $Q$ is the empirical quantile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Random_Lipschitz_Partioner(Min_data_size_percentage,q_in, X_train_in,y_train_in, CV_folds_failsafe, min_size):\n",
    "       \n",
    "    #-----------------------#\n",
    "    # Reset Seed Internally #\n",
    "    #-----------------------#\n",
    "    random.seed(2020)\n",
    "    np.random.seed(2020)\n",
    "\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    # 1) Sample radius from unifom distribution #\n",
    "    #-------------------------------------------#\n",
    "    #-------------------------------------------#\n",
    "    alpha = np.random.uniform(low=.25,high=.5,size=1)[0]\n",
    "\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    # 2) Apply Random Bijection (Shuffle) #\n",
    "    #-------------------------------------#\n",
    "    #-------------------------------------#\n",
    "    X_train_in_shuffled = X_train_in#.sample(frac=1)\n",
    "    y_train_in_shuffled = y_train_in#.sample(frac=1)\n",
    "\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # X) Initializations #\n",
    "    #--------------------#\n",
    "    #--------------------#\n",
    "    # Compute-data-driven radius\n",
    "    Delta_X = distance_matrix(X_train_in_shuffled,X_train_in_shuffled)[::,0]\n",
    "    # note this operation:',distance_matrix(X_train_in_shuffled,X_train_in_shuffled).shape)\n",
    "    print('size of Data driven radius:',Delta_X.shape)\n",
    "    Delta_in = np.quantile(Delta_X,q_in)\n",
    "\n",
    "    # Initialize Random Radius\n",
    "    rand_radius = Delta_in*alpha\n",
    "\n",
    "    # Initialize Data_sizes & ratios\n",
    "    N_tot = X_train_in.shape[0] #<- Total number of data-points in input data-set!\n",
    "    N_radios = np.array([])\n",
    "    N_pool_train_loop = N_tot\n",
    "    # Initialize List of Dataframes\n",
    "    X_internal_train_list = list()\n",
    "    y_internal_train_list = list()\n",
    "\n",
    "    # Initialize Partioned Data-pool\n",
    "    X_internal_train_pool = X_train_in_shuffled\n",
    "    y_internal_train_pool = y_train_in_shuffled\n",
    "\n",
    "    # Initialize counter \n",
    "    part_current_loop = 0\n",
    "\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "    # 3) Iteratively Build Parts #\n",
    "    #----------------------------#\n",
    "    #----------------------------#\n",
    "\n",
    "    while ((N_pool_train_loop/N_tot > Min_data_size_percentage) or (X_internal_train_pool.empty == False)):\n",
    "        # Extract Current Center\n",
    "        center_loop = X_internal_train_pool.iloc[0]\n",
    "        # Compute Distances\n",
    "        ## Training\n",
    "        distances_pool_loop_train = X_internal_train_pool.sub(center_loop)\n",
    "        distances_pool_loop_train = np.array(np.sqrt(np.square(distances_pool_loop_train).sum(axis=1)))\n",
    "        # Evaluate which Distances are less than the given random radius\n",
    "        Part_train_loop = X_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "        Part_train_loop_y = y_internal_train_pool[distances_pool_loop_train<rand_radius]\n",
    "\n",
    "        # Remove all data-points which are \"too small\"\n",
    "        if X_internal_train_pool.shape[0] > max(CV_folds,4):\n",
    "            # Append Current part to list\n",
    "            X_internal_train_list.append(Part_train_loop)\n",
    "            y_internal_train_list.append(Part_train_loop_y)\n",
    "\n",
    "        # Remove current part from pool \n",
    "        X_internal_train_pool = X_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "        y_internal_train_pool = y_internal_train_pool[(np.logical_not(distances_pool_loop_train<rand_radius))]\n",
    "\n",
    "        # Update Current size of pool of training data\n",
    "        N_pool_train_loop = X_internal_train_pool.shape[0]\n",
    "        N_radios = np.append(N_radios,(N_pool_train_loop/N_tot))\n",
    "\n",
    "        # Update Counter\n",
    "        part_current_loop = part_current_loop +1\n",
    "        \n",
    "        # Update User\n",
    "        print('pool train loop percentage:',(N_pool_train_loop/N_tot))\n",
    "\n",
    "\n",
    "    # Post processing #\n",
    "    #-----------------#\n",
    "    # Remove Empty Partitions\n",
    "    N_radios = N_radios[N_radios>0]\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------------------#\n",
    "    # Combine parts which are too small to perform CV without an error\n",
    "    #-----------------------------------------------------------------#\n",
    "    # Initialize lists (partitions) with \"enough\" datums per part\n",
    "    X_internal_train_list_good = list()\n",
    "    y_internal_train_list_good = list()\n",
    "    X_small_parts = list()\n",
    "    y_small_parts = list()\n",
    "    # Initialize first list item test\n",
    "    is_first = True\n",
    "    # Initialize counter\n",
    "    goods_counter = 0\n",
    "    for search_i in range(len(X_internal_train_list)):\n",
    "        number_of_instances_in_part = len(X_internal_train_list[search_i]) \n",
    "        if number_of_instances_in_part < max(CV_folds_failsafe,min_size):\n",
    "            # Check if first \n",
    "            if is_first:\n",
    "                # Initialize set of small X_parts\n",
    "                X_small_parts = X_internal_train_list[search_i]\n",
    "                # Initialize set of small y_parts\n",
    "                y_small_parts = y_internal_train_list[search_i]\n",
    "\n",
    "                # Set is_first to false\n",
    "                is_first = False\n",
    "            else:\n",
    "                X_small_parts = X_small_parts.append(X_internal_train_list[search_i])\n",
    "                #y_small_parts = np.append(y_small_parts,y_internal_train_list[search_i])\n",
    "                y_small_parts = y_small_parts.append(y_internal_train_list[search_i])\n",
    "        else:\n",
    "            # Append to current list\n",
    "            X_internal_train_list_good.append(X_internal_train_list[search_i])\n",
    "            y_internal_train_list_good.append(y_internal_train_list[search_i])\n",
    "            # Update goods counter \n",
    "            goods_counter = goods_counter +1\n",
    "\n",
    "    # Append final one to good list\n",
    "    X_internal_train_list_good.append(X_small_parts)\n",
    "    y_internal_train_list_good.append(y_small_parts)\n",
    "\n",
    "    # reset is_first to false (inscase we want to re-run this particular block)\n",
    "    is_first = True\n",
    "\n",
    "    # Set good lists to regular lists\n",
    "    X_internal_train_list = X_internal_train_list_good\n",
    "    y_internal_train_list = y_internal_train_list_good\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Return Value #\n",
    "    #--------------#\n",
    "    return [X_internal_train_list, y_internal_train_list, N_radios]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Random Partitioner to the given Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "partitioning_time_begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Option_Function == 'SnP':\n",
    "    q_in_auto = .8\n",
    "    Min_data_size_percentage_auto = .1\n",
    "    min_size_part = 100\n",
    "else:\n",
    "    if Option_Function == 'crypto':\n",
    "        q_in_auto = .99\n",
    "        Min_data_size_percentage_auto = .3\n",
    "        min_size_part = 100\n",
    "    if Option_Function == 'Motivational_Example':\n",
    "        q_in_auto = .5\n",
    "        Min_data_size_percentage_auto = .5\n",
    "        min_size_part = 10\n",
    "        # Partition Based on Y\n",
    "        holder_temp = data_y\n",
    "        data_y = X_train\n",
    "        X_train = holder_temp\n",
    "    if Option_Function == 'lorenz':\n",
    "        q_in_auto = .5\n",
    "        Min_data_size_percentage_auto = .5 #.3\n",
    "        min_size_part = 15000 #7000\n",
    "    else:\n",
    "        q_in_auto = .5\n",
    "        Min_data_size_percentage_auto = .3\n",
    "        min_size_part = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of Data driven radius: (43199,)\n",
      "pool train loop percentage: 0.9865274659135628\n",
      "pool train loop percentage: 0.9626843213963286\n",
      "pool train loop percentage: 0.9397902729229843\n",
      "pool train loop percentage: 0.9251834533206788\n",
      "pool train loop percentage: 0.9134702192180375\n",
      "pool train loop percentage: 0.8999050903956111\n",
      "pool train loop percentage: 0.867913609111322\n",
      "pool train loop percentage: 0.33966064029259935\n",
      "pool train loop percentage: 0.2664413528090928\n",
      "pool train loop percentage: 0.0055093867913609115\n",
      "pool train loop percentage: 0.003935276279543508\n",
      "pool train loop percentage: 0.003009328919650918\n",
      "pool train loop percentage: 0.0018055973517905506\n",
      "pool train loop percentage: 0.0003009328919650918\n",
      "pool train loop percentage: 2.3148683997314753e-05\n",
      "pool train loop percentage: 0.0\n",
      "The_parts_listhe number of parts in the input are: 2.\n",
      "X_parts_list: [              x          y          z\n",
      "53    -6.839236 -11.119703  31.736388\n",
      "54    -7.228739 -10.750087  31.660895\n",
      "55    -7.545777 -10.376179  31.598310\n",
      "56    -7.796951 -10.000365  31.538519\n",
      "57    -7.988473  -9.625315  31.473230\n",
      "...         ...        ...        ...\n",
      "43194 -8.092295  -4.927387  30.282290\n",
      "43195 -7.780484  -4.714503  29.862659\n",
      "43196 -7.480086  -4.542631  29.425034\n",
      "43197 -7.193820  -4.409718  28.974508\n",
      "43198 -6.923939  -4.313410  28.515588\n",
      "\n",
      "[22820 rows x 3 columns],                x          y          z\n",
      "0       5.894076  -6.415820  -8.194456\n",
      "1       4.821934  -4.429324  -8.263782\n",
      "2       4.022918  -2.797976  -8.203039\n",
      "3       3.441617  -1.433997  -8.064589\n",
      "4       3.035476  -0.265226  -7.879283\n",
      "...          ...        ...        ...\n",
      "36024  -7.133620  15.368830  41.766037\n",
      "36846  -8.745221  12.623038  43.021892\n",
      "40033  -8.550749  12.040661  42.445774\n",
      "40834 -10.428636  13.210012  45.516945\n",
      "40835  -8.107880  14.581092  43.047755\n",
      "\n",
      "[20378 rows x 3 columns]]\n",
      "The_parts_listhe number of parts in the output are: 2.\n",
      "y_parts_list: [              x          y          z\n",
      "53    -7.228739 -10.750087  31.660895\n",
      "54    -7.545777 -10.376179  31.598310\n",
      "55    -7.796951 -10.000365  31.538519\n",
      "56    -7.988473  -9.625315  31.473230\n",
      "57    -8.126233  -9.253953  31.395850\n",
      "...         ...        ...        ...\n",
      "43194  8.863777   9.073592  27.197825\n",
      "43195  8.882687   9.050673  27.275690\n",
      "43196  8.897304   9.021341  27.350710\n",
      "43197  8.907440   8.985900  27.422011\n",
      "43198  8.912959   8.944733  27.488760\n",
      "\n",
      "[22820 rows x 3 columns],                x         y          z\n",
      "0       4.821934 -4.429324  -8.263782\n",
      "1       4.022918 -2.797976  -8.203039\n",
      "2       3.441617 -1.433997  -8.064589\n",
      "3       3.035476 -0.265226  -7.879283\n",
      "4       2.772190  0.767175  -7.664677\n",
      "...          ...       ...        ...\n",
      "36024  -8.532483  5.053128  38.584164\n",
      "36846   6.071259  1.200728  32.845977\n",
      "40033  11.761765  4.486235  37.771384\n",
      "40834  -5.206357  5.048127  34.144523\n",
      "40835  -4.219409  5.259560  33.006513\n",
      "\n",
      "[20378 rows x 3 columns]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Number of Parts currently generated\n",
    "N_parts_generated = 0\n",
    "\n",
    "# Generate Partition (with option to regenerate if only 1 part is randomly produced)\n",
    "while N_parts_generated < 2:\n",
    "    # Generate Parts\n",
    "    X_parts_list, y_parts_list, N_ratios = Random_Lipschitz_Partioner(Min_data_size_percentage=Min_data_size_percentage_auto, \n",
    "                                                                      q_in=q_in_auto, \n",
    "                                                                      X_train_in=X_train, \n",
    "                                                                      y_train_in=data_y, \n",
    "                                                                      CV_folds_failsafe=CV_folds,\n",
    "                                                                      min_size = min_size_part)\n",
    "    \n",
    "    # Update Number of Parts\n",
    "    N_parts_generated = len(X_parts_list)\n",
    "    # Shuffle hyperparameters\n",
    "    Min_data_size_percentage_auto = (Min_data_size_percentage_auto + random.uniform(0,.3)) % 1\n",
    "    q_in_auto = (q_in_auto + random.uniform(0,.3)) % 1\n",
    "    \n",
    "    # Update User\n",
    "    print('The_parts_listhe number of parts in the input are: ' + str(len(X_parts_list))+'.')\n",
    "    print('X_parts_list:', X_parts_list)\n",
    "    print('The_parts_listhe number of parts in the output are: ' + str(len(y_parts_list))+'.')\n",
    "    print('y_parts_list:', y_parts_list)\n",
    "    \n",
    "# Trash removal (removes empty parts)\n",
    "X_parts_list = list(filter(([]).__ne__, X_parts_list))\n",
    "y_parts_list = list(filter(([]).__ne__, y_parts_list))\n",
    "    \n",
    "    \n",
    "# ICML Rebuttle Deadline = Coersion!\n",
    "if Option_Function == 'Motivational_Example':\n",
    "    # Flipback After Partitioning Based on Y (since code was made for partitioning in X!)\n",
    "    holder_temp = data_y\n",
    "    data_y = X_train\n",
    "    X_train = holder_temp\n",
    "    holder_temp = y_parts_list\n",
    "    y_parts_list = X_parts_list\n",
    "    X_parts_list = holder_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_time = time.time() - partitioning_time_begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_parts_listhe number of parts are: 2.\n"
     ]
    }
   ],
   "source": [
    "print('The_parts_listhe number of parts are: ' + str(len(X_parts_list))+'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHECK:                x          y          z\n",
      "0       5.894076  -6.415820  -8.194456\n",
      "1       4.821934  -4.429324  -8.263782\n",
      "2       4.022918  -2.797976  -8.203039\n",
      "3       3.441617  -1.433997  -8.064589\n",
      "4       3.035476  -0.265226  -7.879283\n",
      "...          ...        ...        ...\n",
      "36024  -7.133620  15.368830  41.766037\n",
      "36846  -8.745221  12.623038  43.021892\n",
      "40033  -8.550749  12.040661  42.445774\n",
      "40834 -10.428636  13.210012  45.516945\n",
      "40835  -8.107880  14.581092  43.047755\n",
      "\n",
      "[20378 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('CHECK:',X_parts_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Training Predictions on each part (each part represent a different manifold)\n",
    "- Train locally (on each \"naive part\")\n",
    "- Generate predictions for (full) training and testings sets respectively, to be used in training the classifer and for prediction, respectively.  \n",
    "- Generate predictions on all of testing-set (will be selected between later using classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[              x          y          z\n",
      "53    -6.839236 -11.119703  31.736388\n",
      "54    -7.228739 -10.750087  31.660895\n",
      "55    -7.545777 -10.376179  31.598310\n",
      "56    -7.796951 -10.000365  31.538519\n",
      "57    -7.988473  -9.625315  31.473230\n",
      "...         ...        ...        ...\n",
      "43194 -8.092295  -4.927387  30.282290\n",
      "43195 -7.780484  -4.714503  29.862659\n",
      "43196 -7.480086  -4.542631  29.425034\n",
      "43197 -7.193820  -4.409718  28.974508\n",
      "43198 -6.923939  -4.313410  28.515588\n",
      "\n",
      "[22820 rows x 3 columns],                x          y          z\n",
      "0       5.894076  -6.415820  -8.194456\n",
      "1       4.821934  -4.429324  -8.263782\n",
      "2       4.022918  -2.797976  -8.203039\n",
      "3       3.441617  -1.433997  -8.064589\n",
      "4       3.035476  -0.265226  -7.879283\n",
      "...          ...        ...        ...\n",
      "36024  -7.133620  15.368830  41.766037\n",
      "36846  -8.745221  12.623038  43.021892\n",
      "40033  -8.550749  12.040661  42.445774\n",
      "40834 -10.428636  13.210012  45.516945\n",
      "40835  -8.107880  14.581092  43.047755\n",
      "\n",
      "[20378 rows x 3 columns]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(X_parts_list)\n",
    "# print dataframes.\n",
    "#X_parts_list\n",
    "#########################################################\n",
    "############## -- save data manifold -- #################\n",
    "#########################################################\n",
    "################ --- manifold 1 train --- #####################\n",
    "GET_PARTITION_manifold1 = pd.DataFrame(X_parts_list[0])#, columns=['xdot', 'ydot', 'zdot'])\n",
    "#GET_PARTITION_manifold1.index.names = ['index']\n",
    "GET_PARTITION_manifold1.to_csv('GET_PARTITION_manifold1.csv')\n",
    "np.savetxt(\"GET_PARTITION_manifold1.csv\", GET_PARTITION_manifold1, delimiter=\",\")\n",
    "################ --- manifold 2 train --- #####################\n",
    "GET_PARTITION_manifold2 = pd.DataFrame(X_parts_list[1])#, columns=['xdot', 'ydot', 'zdot'])\n",
    "#GET_PARTITION_manifold2.index.names = ['index']\n",
    "GET_PARTITION_manifold2.to_csv('GET_PARTITION_manifold2.csv')\n",
    "np.savetxt(\"GET_PARTITION_manifold2.csv\", GET_PARTITION_manifold2, delimiter=\",\")\n",
    "################ --- manifold 3 train --- #####################\n",
    "#Architope_manifold3_prediction_y_train = pd.DataFrame(Architope_manifolds_prediction_y_train[:,:,2], columns=['xdot', 'ydot', 'zdot'])\n",
    "#Architope_manifold3_prediction_y_train.index.names = ['index']\n",
    "#Architope_manifold3_prediction_y_train.to_csv('lorenz_PCNN_manifold3.csv')\n",
    "#np.savetxt(\"lorenz_PCNN_manifold3.csv\", Architope_manifold3_prediction_y_train, delimiter=\",\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
